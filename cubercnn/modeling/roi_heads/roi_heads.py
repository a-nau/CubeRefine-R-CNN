# Copyright (c) Meta Platforms, Inc. and affiliates
import logging
from datetime import datetime
from typing import Dict, List, Tuple

import cv2
import einops
import numpy as np
import torch
import torch.nn.functional as F
from detectron2.config import configurable
from detectron2.layers import ShapeSpec, nonzero_tuple
from detectron2.modeling.poolers import ROIPooler
from detectron2.modeling.proposal_generator.proposal_utils import (
    add_ground_truth_to_proposals,
)
from detectron2.modeling.roi_heads import (
    ROI_HEADS_REGISTRY,
    StandardROIHeads,
    select_foreground_proposals,
)
from detectron2.structures import Boxes, BoxMode, Instances, pairwise_ioa, pairwise_iou
from detectron2.utils.events import get_event_storage
from pytorch3d.transforms.so3 import so3_relative_angle
from torch import nn

import cubercnn.util.shape as shape_utils
from cubercnn import util
from cubercnn.evaluation.pix3d_evaluation import transform_meshes_to_camera_coord_system
from cubercnn.modeling.proposal_generator.rpn import subsample_labels
from cubercnn.modeling.roi_heads.cube_head import build_cube_head
from cubercnn.modeling.roi_heads.fast_rcnn import FastRCNNOutputs
from cubercnn.modeling.roi_heads.mesh_head import build_mesh_head, mesh_rcnn_loss
from cubercnn.structures.mesh import (
    Meshes,
    MeshInstances,
    batch_crop_meshes_within_box,
    gt_bbox3d_as_init_per_batch,
)

logger = logging.getLogger(__name__)

E_CONSTANT = 2.71828183
SQRT_2_CONSTANT = 1.41421356


def build_roi_heads(cfg, input_shape, priors=None):
    """
    Build ROIHeads defined by `cfg.MODEL.ROI_HEADS.NAME`.
    """
    name = cfg.MODEL.ROI_HEADS.NAME
    return ROI_HEADS_REGISTRY.get(name)(cfg, input_shape, priors=priors)


@ROI_HEADS_REGISTRY.register()
class ROIHeads3D(StandardROIHeads):
    @configurable
    def __init__(
        self,
        *,
        ignore_thresh: float,
        cube_on: bool,
        cube_head: nn.Module,
        cube_pooler: nn.Module,
        loss_w_3d: float,
        loss_w_xy: float,
        loss_w_z: float,
        loss_w_dims: float,
        loss_w_pose: float,
        loss_w_joint: float,
        use_confidence: float,
        inverse_z_weight: bool,
        z_type: str,
        pose_type: str,
        cluster_bins: int,
        mesh_on: bool = False,
        mesh_head: nn.Module = None,
        mesh_pooler: nn.Module = None,
        priors=None,
        dims_priors_enabled=None,
        disentangled_loss=None,
        virtual_depth=None,
        virtual_focal=None,
        test_scale=None,
        allocentric_pose=None,
        chamfer_pose=None,
        scale_roi_boxes=None,
        # Mesh Head
        chamfer_loss_weight=None,
        normals_loss_weight=None,
        edge_loss_weight=None,
        gt_num_samples=None,
        pred_num_samples=None,
        gt_coord_thresh=None,
        cuboid_mesh_division_levels=4,  # necessary for mesh eval
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.scale_roi_boxes = scale_roi_boxes

        # rotation settings
        self.allocentric_pose = allocentric_pose
        self.chamfer_pose = chamfer_pose

        # virtual settings
        self.virtual_depth = virtual_depth
        self.virtual_focal = virtual_focal

        # loss weights, <=0 is off
        self.loss_w_3d = loss_w_3d
        self.loss_w_xy = loss_w_xy
        self.loss_w_z = loss_w_z
        self.loss_w_dims = loss_w_dims
        self.loss_w_pose = loss_w_pose
        self.loss_w_joint = loss_w_joint

        # loss modes
        self.disentangled_loss = disentangled_loss
        self.inverse_z_weight = inverse_z_weight

        # misc
        self.cube_on = cube_on
        self.test_scale = test_scale
        self.ignore_thresh = ignore_thresh

        # related to network outputs
        self.z_type = z_type
        self.pose_type = pose_type
        self.use_confidence = use_confidence

        # related to priors
        self.cluster_bins = cluster_bins
        self.dims_priors_enabled = dims_priors_enabled

        # if there is no 3D loss, then we don't need any heads.
        if self.cube_on and loss_w_3d > 0:

            self.cube_head = cube_head
            self.cube_pooler = cube_pooler

            # the dimensions could rely on pre-computed priors
            if self.dims_priors_enabled and priors is not None:
                self.priors_dims_per_cat = nn.Parameter(
                    torch.FloatTensor(priors["priors_dims_per_cat"]).unsqueeze(0)
                )
            else:
                self.priors_dims_per_cat = nn.Parameter(
                    torch.ones(1, self.num_classes, 2, 3)
                )

            # Optionally, refactor priors and store them in the network params
            if self.cluster_bins > 1 and priors is not None:

                # the depth could have been clustered based on 2D scales
                priors_z_scales = torch.stack(
                    [torch.FloatTensor(prior[1]) for prior in priors["priors_bins"]]
                )
                self.priors_z_scales = nn.Parameter(priors_z_scales)

            else:
                self.priors_z_scales = nn.Parameter(
                    torch.ones(self.num_classes, self.cluster_bins)
                )

            # the depth can be based on priors
            if self.z_type == "priors":

                assert (
                    self.cluster_bins > 1
                ), "To use z_type of priors, there must be more than 1 cluster bin"

                if priors is None:
                    self.priors_z_stats = nn.Parameter(
                        torch.ones(self.num_classes, self.cluster_bins, 2).float()
                    )
                else:

                    # stats
                    priors_z_stats = torch.cat(
                        [
                            torch.FloatTensor(prior[2]).unsqueeze(0)
                            for prior in priors["priors_bins"]
                        ]
                    )
                    self.priors_z_stats = nn.Parameter(priors_z_stats)

            # Mesh Head
            self.cuboid_mesh_division_levels = cuboid_mesh_division_levels
            self.mesh_on = mesh_on
            if self.mesh_on:
                self.mesh_head = mesh_head
                self.mesh_pooler = mesh_pooler
                self.chamfer_loss_weight = chamfer_loss_weight
                self.normals_loss_weight = normals_loss_weight
                self.edge_loss_weight = edge_loss_weight
                self.gt_num_samples = gt_num_samples
                self.pred_num_samples = pred_num_samples
                self.gt_coord_thresh = gt_coord_thresh

    @classmethod
    def from_config(cls, cfg, input_shape: Dict[str, ShapeSpec], priors=None):

        ret = super().from_config(cfg, input_shape)

        # pass along priors
        ret["box_predictor"] = FastRCNNOutputs(cfg, ret["box_head"].output_shape)
        ret.update(cls._init_cube_head(cfg, input_shape))
        ret.update(cls._init_mesh_head(cfg, input_shape))
        ret["priors"] = priors

        return ret

    @classmethod
    def _init_cube_head(self, cfg, input_shape: Dict[str, ShapeSpec]):

        cube_on = cfg.MODEL.CUBE_ON
        if not cube_on:
            return {}
        in_features = cfg.MODEL.ROI_HEADS.IN_FEATURES
        pooler_scales = tuple(1.0 / input_shape[k].stride for k in in_features)
        pooler_resolution = cfg.MODEL.ROI_CUBE_HEAD.POOLER_RESOLUTION
        pooler_sampling_ratio = cfg.MODEL.ROI_CUBE_HEAD.POOLER_SAMPLING_RATIO
        pooler_type = cfg.MODEL.ROI_CUBE_HEAD.POOLER_TYPE

        cube_pooler = ROIPooler(
            output_size=pooler_resolution,
            scales=pooler_scales,
            sampling_ratio=pooler_sampling_ratio,
            pooler_type=pooler_type,
        )

        in_channels = [input_shape[f].channels for f in in_features][0]
        shape = ShapeSpec(
            channels=in_channels, width=pooler_resolution, height=pooler_resolution
        )

        cube_head = build_cube_head(cfg, shape)

        return {
            "cube_on": cube_on,
            "cube_head": cube_head,
            "cube_pooler": cube_pooler,
            "use_confidence": cfg.MODEL.ROI_CUBE_HEAD.USE_CONFIDENCE,
            "inverse_z_weight": cfg.MODEL.ROI_CUBE_HEAD.INVERSE_Z_WEIGHT,
            "loss_w_3d": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_3D,
            "loss_w_xy": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_XY,
            "loss_w_z": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_Z,
            "loss_w_dims": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_DIMS,
            "loss_w_pose": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_POSE,
            "loss_w_joint": cfg.MODEL.ROI_CUBE_HEAD.LOSS_W_JOINT,
            "z_type": cfg.MODEL.ROI_CUBE_HEAD.Z_TYPE,
            "pose_type": cfg.MODEL.ROI_CUBE_HEAD.POSE_TYPE,
            "dims_priors_enabled": cfg.MODEL.ROI_CUBE_HEAD.DIMS_PRIORS_ENABLED,
            "disentangled_loss": cfg.MODEL.ROI_CUBE_HEAD.DISENTANGLED_LOSS,
            "virtual_depth": cfg.MODEL.ROI_CUBE_HEAD.VIRTUAL_DEPTH,
            "virtual_focal": cfg.MODEL.ROI_CUBE_HEAD.VIRTUAL_FOCAL,
            "test_scale": cfg.INPUT.MIN_SIZE_TEST,
            "chamfer_pose": cfg.MODEL.ROI_CUBE_HEAD.CHAMFER_POSE,
            "allocentric_pose": cfg.MODEL.ROI_CUBE_HEAD.ALLOCENTRIC_POSE,
            "cluster_bins": cfg.MODEL.ROI_CUBE_HEAD.CLUSTER_BINS,
            "ignore_thresh": cfg.MODEL.RPN.IGNORE_THRESHOLD,
            "scale_roi_boxes": cfg.MODEL.ROI_CUBE_HEAD.SCALE_ROI_BOXES,
        }

    @classmethod
    def _init_mesh_head(cls, cfg, input_shape):
        mesh_on = cfg.MODEL.MESH_ON
        if not mesh_on:
            return {}
        in_features = cfg.MODEL.ROI_HEADS.IN_FEATURES
        in_channels = [input_shape[f].channels for f in in_features][0]

        mesh_pooler_resolution = cfg.MODEL.ROI_MESH_HEAD.POOLER_RESOLUTION  # 14
        mesh_pooler_scales = tuple(1.0 / input_shape[k].stride for k in in_features)
        mesh_sampling_ratio = cfg.MODEL.ROI_MESH_HEAD.POOLER_SAMPLING_RATIO
        mesh_pooler_type = cfg.MODEL.ROI_MESH_HEAD.POOLER_TYPE

        # alignment of region
        mesh_pooler = ROIPooler(
            output_size=mesh_pooler_resolution,
            scales=mesh_pooler_scales,
            sampling_ratio=mesh_sampling_ratio,
            pooler_type=mesh_pooler_type,
        )
        mesh_head = build_mesh_head(
            cfg,
            ShapeSpec(
                channels=in_channels,
                height=mesh_pooler_resolution,
                width=mesh_pooler_resolution,
            ),
        )
        return {
            "mesh_pooler": mesh_pooler,
            "mesh_head": mesh_head,
            "mesh_on": mesh_on,
            "chamfer_loss_weight": cfg.MODEL.ROI_MESH_HEAD.CHAMFER_LOSS_WEIGHT,
            "normals_loss_weight": cfg.MODEL.ROI_MESH_HEAD.NORMALS_LOSS_WEIGHT,
            "edge_loss_weight": cfg.MODEL.ROI_MESH_HEAD.EDGE_LOSS_WEIGHT,
            "gt_num_samples": cfg.MODEL.ROI_MESH_HEAD.GT_NUM_SAMPLES,
            "pred_num_samples": cfg.MODEL.ROI_MESH_HEAD.PRED_NUM_SAMPLES,
            "gt_coord_thresh": cfg.MODEL.ROI_MESH_HEAD.GT_COORD_THRESH,
            "cuboid_mesh_division_levels": cfg.MODEL.ROI_MESH_HEAD.CUBOID_MESH_DIVISION_LEVELS,
        }

    def forward(self, images, features, proposals, Ks, im_scales_ratio, targets=None):

        im_dims = [image.shape[1:] for image in images]

        del images

        if self.training:
            proposals = self.label_and_sample_proposals(proposals, targets)

        del targets

        if self.training:

            losses = self._forward_box(features, proposals)
            if self.loss_w_3d > 0:
                res = self._forward_cube(
                    features, proposals, Ks, im_dims, im_scales_ratio
                )
                if len(res) == 2:
                    cube_losses, pred_boxes3d = res
                elif len(res) == 1:
                    instances = res
                else:
                    ValueError("Unexcepted number of return values")

                losses.update(cube_losses)
                if self.mesh_on:
                    mesh_losses, pred_meshes = self._forward_mesh(
                        pred_boxes3d, features, proposals, Ks, im_dims, im_scales_ratio
                    )
                    losses.update(mesh_losses)

            return [], losses

        else:

            # when oracle is available, by pass the box forward.
            # simulate the predicted instances by creating a new
            # instance for each passed in image.
            if isinstance(proposals, list) and ~np.any(
                [isinstance(p, Instances) for p in proposals]
            ):
                pred_instances = []
                for proposal, im_dim in zip(proposals, im_dims):

                    pred_instances_i = Instances(im_dim)
                    pred_instances_i.pred_boxes = Boxes(proposal["gt_bbox2D"])
                    pred_instances_i.pred_classes = proposal["gt_classes"]
                    pred_instances_i.scores = torch.ones_like(
                        proposal["gt_classes"]
                    ).float()
                    pred_instances.append(pred_instances_i)
            else:
                pred_instances = self._forward_box(features, proposals)

            if self.loss_w_3d > 0:
                pred_instances = self._forward_cube(
                    features, pred_instances, Ks, im_dims, im_scales_ratio
                )
                if any([len(i) == 0 for i in pred_instances]):
                    # handle special case when no relevant proposal was found
                    return pred_instances, {}
                pred_boxes3d = [
                    torch.stack([i.pred_bbox3D for i in pred_instances]).squeeze(0),
                    torch.stack([i.pred_bbox3D_faces for i in pred_instances]).squeeze(
                        0
                    ),
                ]  # assemble boxes3d for forward
                pred_instances = self._forward_mesh(
                    pred_boxes3d,
                    features,
                    pred_instances,
                    Ks,
                    im_dims,
                    im_scales_ratio,
                )
            return pred_instances, {}

    def _forward_mesh(
        self, pred_boxes3d, features, instances, Ks, im_current_dims, im_scales_ratio
    ):
        device = pred_boxes3d[0].device
        features = [features[f] for f in self.in_features]

        # training on foreground
        if self.training:

            losses = {}

            # add up the amount we should normalize the losses by.
            # this follows the same logic as the BoxHead, where each FG proposal
            # is able to contribute the same amount of supervision. Technically,
            # this value doesn't change during training unless the batch size is dynamic.
            self.normalize_factor = max(
                sum([i.gt_classes.numel() for i in instances]), 1.0
            )

            # The loss is only defined on positive proposals
            proposals, _ = select_foreground_proposals(instances, self.num_classes)
            proposal_boxes = [x.proposal_boxes for x in proposals]
            pred_boxes = [x.pred_boxes for x in proposals]

        # eval on all instances
        else:
            proposals = instances
            pred_boxes = [x.pred_boxes for x in instances]
            proposal_boxes = pred_boxes

        proposal_boxes_scaled = self.scale_proposals(proposal_boxes)
        for proposal_i, proposal_boxes_scaled_i in zip(
            proposals, proposal_boxes_scaled
        ):
            proposal_i.proposal_boxes_scaled = proposal_boxes_scaled_i

        if self.mesh_on:
            # forward features
            mesh_features = self.mesh_pooler(features, proposal_boxes_scaled)
            n = mesh_features.shape[0]
        else:
            n = len(proposal_boxes_scaled)

        # nothing to do..
        if n == 0:
            return instances

        num_boxes_per_image = [len(i) for i in proposals]

        verts, faces = pred_boxes3d
        init_meshes = gt_bbox3d_as_init_per_batch(
            verts, faces, level=self.cuboid_mesh_division_levels, device=device
        )
        assert sum(num_boxes_per_image) == len(
            init_meshes
        ), "2D Bboxes and Mesh Preds do not coiincide!"

        # transform mesh into unit cube
        _Ks = torch.tensor(
            sum(
                [
                    [[k[0, 0], k[0, 2], k[1, 2]]] * len(proposal_boxes_scaled[i])
                    for i, k in enumerate(Ks)
                ],
                [],
            ),
            device=device,
            dtype=Ks[0].dtype,
        )
        _unique_Ks = torch.tensor(
            [[k[0, 0], k[0, 2], k[1, 2]] for k in Ks],
            device=device,
            dtype=Ks[0].dtype,
        )
        init_meshes = MeshInstances(
            [
                (v, f)
                for (v, f) in zip(init_meshes.verts_list(), init_meshes.faces_list())
            ]
        )
        bboxes = torch.vstack([p.tensor for p in proposal_boxes_scaled]).to(device)
        with torch.no_grad():
            # note: batch crop yielded different results for train/inference due to differences in bboxes!
            init_meshes = batch_crop_meshes_within_box(init_meshes, bboxes, _Ks)

        if self.mesh_on:
            # Pred
            pred_meshes = self.mesh_head(
                mesh_features, init_meshes
            )  # one list for each refinement stage

            # loss weights
            loss_weights = {
                "chamfer": self.chamfer_loss_weight,
                "normals": self.normals_loss_weight,
                "edge": self.edge_loss_weight,
            }
        else:
            pred_meshes = [init_meshes]  # to mimick stages

        if not pred_meshes[0].isempty():
            if self.training:
                loss_chamfer, loss_normals, loss_edge, target_meshes = mesh_rcnn_loss(
                    pred_meshes,
                    proposals,
                    _unique_Ks,
                    loss_weights=loss_weights,
                    gt_num_samples=self.gt_num_samples,
                    pred_num_samples=self.pred_num_samples,
                    gt_coord_thresh=self.gt_coord_thresh,
                )

                return {
                    "Mesh/chamfer_loss": loss_chamfer,
                    "Mesh/normals_loss": loss_normals,
                    "Mesh/edge_loss": loss_edge,
                }, pred_meshes
            else:
                # Choose final meshes only and transform onto original position
                pred_meshes_final = pred_meshes[-1]  # only use last stage
                zranges = torch.stack(
                    [torch.tensor([v[..., 2].min(), v[..., 2].max()]) for v in verts]
                ).to(device)
                idx = 0
                for i, instances_i in enumerate(instances):
                    # Note: need to map instances onto other values
                    idx_start = idx
                    idx += num_boxes_per_image[i]
                    idx_end = idx
                    instances_i.pred_meshes = pred_meshes_final[
                        idx_start:idx_end
                    ].clone()
                    pred_meshes_i = transform_meshes_to_camera_coord_system(
                        meshes=pred_meshes_final[idx_start:idx_end].clone(),
                        boxes=bboxes[idx_start:idx_end],
                        zranges=zranges[idx_start:idx_end],
                        Ks=_Ks[idx_start:idx_end],
                        imsize=im_current_dims[i],  # per instance
                        adjust_cs=True,
                    )
                    instances_i.pred_meshes_world = pred_meshes_i

                return instances

    def _forward_box(
        self, features: Dict[str, torch.Tensor], proposals: List[Instances]
    ):
        """
        Forward logic of the box prediction branch. If `self.train_on_pred_boxes is True`,
            the function puts predicted boxes in the `proposal_boxes` field of `proposals` argument.

        Args:
            features (dict[str, Tensor]): mapping from feature map names to tensor.
                Same as in :meth:`ROIHeads.forward`.
            proposals (list[Instances]): the per-image object proposals with
                their matching ground truth.
                Each has fields "proposal_boxes", and "objectness_logits",
                "gt_classes", "gt_boxes".

        Returns:
            In training, a dict of losses.
            In inference, a list of `Instances`, the predicted instances.
        """
        features = [features[f] for f in self.box_in_features]
        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])
        box_features = self.box_head(box_features)
        predictions = self.box_predictor(box_features)
        del box_features

        if self.training:
            losses = self.box_predictor.losses(
                predictions,
                proposals,
            )
            pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(
                predictions, proposals
            )
            for proposals_per_image, pred_boxes_per_image in zip(proposals, pred_boxes):
                proposals_per_image.pred_boxes = Boxes(pred_boxes_per_image)

            # proposals is modified in-place below, so losses must be computed first.
            if self.train_on_pred_boxes:
                with torch.no_grad():
                    pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(
                        predictions, proposals
                    )
                    for proposals_per_image, pred_boxes_per_image in zip(
                        proposals, pred_boxes
                    ):
                        proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)
            return losses
        else:
            pred_instances, _ = self.box_predictor.inference(
                predictions,
                proposals,
            )
            return pred_instances

    def l1_loss(self, vals, target):
        return F.smooth_l1_loss(vals, target, reduction="none", beta=0.0)

    def chamfer_loss(self, vals, target):
        B = vals.shape[0]
        xx = vals.view(B, 8, 1, 3)
        yy = target.view(B, 1, 8, 3)
        l1_dist = (xx - yy).abs().sum(-1)
        l1 = l1_dist.min(1).values.mean(-1) + l1_dist.min(2).values.mean(-1)
        return l1

    # optionally, scale proposals to zoom RoI in (<1.0) our out (>1.0)
    def scale_proposals(self, proposal_boxes):
        if self.scale_roi_boxes > 0:

            proposal_boxes_scaled = []
            for boxes in proposal_boxes:
                centers = boxes.get_centers()
                widths = boxes.tensor[:, 2] - boxes.tensor[:, 0]
                heights = boxes.tensor[:, 2] - boxes.tensor[:, 0]
                x1 = centers[:, 0] - 0.5 * widths * self.scale_roi_boxes
                x2 = centers[:, 0] + 0.5 * widths * self.scale_roi_boxes
                y1 = centers[:, 1] - 0.5 * heights * self.scale_roi_boxes
                y2 = centers[:, 1] + 0.5 * heights * self.scale_roi_boxes
                boxes_scaled = Boxes(torch.stack([x1, y1, x2, y2], dim=1))
                proposal_boxes_scaled.append(boxes_scaled)
        else:
            proposal_boxes_scaled = proposal_boxes

        return proposal_boxes_scaled

    def _forward_cube(self, features, instances, Ks, im_current_dims, im_scales_ratio):

        device = list(features.values())[0].device
        features = [features[f] for f in self.in_features]

        # training on foreground
        if self.training:

            losses = {}

            # add up the amount we should normalize the losses by.
            # this follows the same logic as the BoxHead, where each FG proposal
            # is able to contribute the same amount of supervision. Technically,
            # this value doesn't change during training unless the batch size is dynamic.
            self.normalize_factor = max(
                sum([i.gt_classes.numel() for i in instances]), 1.0
            )

            # The loss is only defined on positive proposals
            proposals, _ = select_foreground_proposals(instances, self.num_classes)
            proposal_boxes = [x.proposal_boxes for x in proposals]
            pred_boxes = [x.pred_boxes for x in proposals]

            box_classes = (
                torch.cat([p.gt_classes for p in proposals], dim=0)
                if len(proposals)
                else torch.empty(0)
            )
            gt_boxes3D = torch.cat(
                [p.gt_boxes3D for p in proposals],
                dim=0,
            )
            gt_poses = torch.cat(
                [p.gt_poses for p in proposals],
                dim=0,
            )

            assert len(gt_poses) == len(gt_boxes3D) == len(box_classes)

        # eval on all instances
        else:
            proposals = instances
            pred_boxes = [x.pred_boxes for x in instances]
            proposal_boxes = pred_boxes
            box_classes = torch.cat([x.pred_classes for x in instances])

        proposal_boxes_scaled = self.scale_proposals(proposal_boxes)

        # forward features
        cube_features = self.cube_pooler(features, proposal_boxes_scaled).flatten(1)

        n = cube_features.shape[0]

        # nothing to do..
        if n == 0:
            return instances

        num_boxes_per_image = [len(i) for i in proposals]

        # scale the intrinsics according to the ratio the image has been scaled.
        # this means the projections at the current scale are in sync.
        Ks_scaled_per_box = torch.cat(
            [
                (Ks[i] / im_scales_ratio[i]).unsqueeze(0).repeat([num, 1, 1])
                for (i, num) in enumerate(num_boxes_per_image)
            ]
        ).to(cube_features.device)
        Ks_scaled_per_box[:, -1, -1] = 1

        focal_lengths_per_box = torch.cat(
            [
                (Ks[i][1, 1]).unsqueeze(0).repeat([num])
                for (i, num) in enumerate(num_boxes_per_image)
            ]
        ).to(cube_features.device)

        im_ratios_per_box = torch.cat(
            [
                torch.FloatTensor([im_scales_ratio[i]]).repeat(num)
                for (i, num) in enumerate(num_boxes_per_image)
            ]
        ).to(cube_features.device)

        # scaling factor for Network resolution -> Original
        im_scales_per_box = torch.cat(
            [
                torch.FloatTensor([im_current_dims[i][0]]).repeat(num)
                for (i, num) in enumerate(num_boxes_per_image)
            ]
        ).to(cube_features.device)

        im_scales_original_per_box = im_scales_per_box * im_ratios_per_box

        if self.virtual_depth:

            virtual_to_real = util.compute_virtual_scale_from_focal_spaces(
                focal_lengths_per_box,
                im_scales_original_per_box,
                self.virtual_focal,
                im_scales_per_box,
            )
            real_to_virtual = 1 / virtual_to_real

        else:
            real_to_virtual = virtual_to_real = 1.0

        # 2D boxes are needed to apply deltas
        src_boxes = torch.cat(
            [box_per_im.tensor for box_per_im in proposal_boxes], dim=0
        )
        src_widths = src_boxes[:, 2] - src_boxes[:, 0]
        src_heights = src_boxes[:, 3] - src_boxes[:, 1]
        src_scales = (src_heights**2 + src_widths**2).sqrt()
        src_ctr_x = src_boxes[:, 0] + 0.5 * src_widths
        src_ctr_y = src_boxes[:, 1] + 0.5 * src_heights

        # For some methods, we need the predicted 2D box,
        # e.g., the differentiable tensors from the 2D box head.
        pred_src_boxes = torch.cat(
            [box_per_im.tensor for box_per_im in pred_boxes], dim=0
        )
        pred_widths = pred_src_boxes[:, 2] - pred_src_boxes[:, 0]
        pred_heights = pred_src_boxes[:, 3] - pred_src_boxes[:, 1]
        pred_src_x = (pred_src_boxes[:, 2] + pred_src_boxes[:, 0]) * 0.5
        pred_src_y = (pred_src_boxes[:, 3] + pred_src_boxes[:, 1]) * 0.5

        # forward predictions
        cube_2d_deltas, cube_z, cube_dims, cube_pose, cube_uncert = self.cube_head(
            cube_features
        )

        # simple indexing re-used commonly for selection purposes
        fg_inds = torch.arange(n)

        # Z when clusters are used
        if cube_z is not None and self.cluster_bins > 1:

            # compute closest bin assignments per batch per category (batch x n_category)
            scales_diff = (
                self.priors_z_scales.detach().T.unsqueeze(0)
                - src_scales.unsqueeze(1).unsqueeze(2)
            ).abs()

            # assign the correct scale prediction.
            # (the others are not used / thrown away)
            assignments = scales_diff.argmin(1)

            # select FG, category, and correct cluster
            cube_z = cube_z[fg_inds, :, box_classes, :][
                fg_inds, assignments[fg_inds, box_classes]
            ]

        elif cube_z is not None:

            # if z is available, collect the per-category predictions.
            cube_z = cube_z[fg_inds, box_classes, :]

        cube_dims = cube_dims[fg_inds, box_classes, :]
        cube_pose = cube_pose[fg_inds, box_classes, :, :]

        if self.use_confidence:

            # if uncertainty is available, collect the per-category predictions.
            cube_uncert = cube_uncert[fg_inds, box_classes]

        cube_2d_deltas = cube_2d_deltas[fg_inds, box_classes, :]

        # apply our predicted deltas based on src boxes.
        cube_x = src_ctr_x + src_widths * cube_2d_deltas[:, 0]
        cube_y = src_ctr_y + src_heights * cube_2d_deltas[:, 1]

        cube_xy = torch.cat((cube_x.unsqueeze(1), cube_y.unsqueeze(1)), dim=1)

        cube_dims_norm = cube_dims
        cube_dims = torch.exp(cube_dims_norm)

        if self.dims_priors_enabled:

            # gather prior dimensions
            prior_dims = self.priors_dims_per_cat.detach().repeat([n, 1, 1, 1])[
                fg_inds, box_classes
            ][:, 0, :]
            cube_dims = cube_dims * prior_dims

        if self.allocentric_pose:

            # To compare with GTs, we need the pose to be egocentric, not allocentric
            cube_pose_allocentric = cube_pose
            cube_pose = util.R_from_allocentric(
                Ks_scaled_per_box, cube_pose, u=cube_x.detach(), v=cube_y.detach()
            )

        cube_z = cube_z.squeeze()

        if self.z_type == "sigmoid":
            cube_z_norm = torch.sigmoid(cube_z)
            cube_z = cube_z_norm * 100

        elif self.z_type == "log":
            cube_z_norm = cube_z
            cube_z = torch.exp(cube_z)

        elif self.z_type == "priors":

            # gather the mean depth, same operation as above, for a n x c result
            z_means = self.priors_z_stats[:, :, 0].T.unsqueeze(0).repeat([n, 1, 1])
            z_means = torch.gather(z_means, 1, assignments.unsqueeze(1)).squeeze(1)

            # gather the std depth, same operation as above, for a n x c result
            z_stds = self.priors_z_stats[:, :, 1].T.unsqueeze(0).repeat([n, 1, 1])
            z_stds = torch.gather(z_stds, 1, assignments.unsqueeze(1)).squeeze(1)

            # do not learn these, they are static
            z_means = z_means.detach()
            z_stds = z_stds.detach()

            z_means = z_means[fg_inds, box_classes]
            z_stds = z_stds[fg_inds, box_classes]

            cube_z_norm = cube_z
            cube_z = (cube_z) * z_stds + z_means

        if self.virtual_depth:
            cube_z = cube_z * virtual_to_real

        if self.training:

            prefix = "Cube/"
            storage = get_event_storage()

            # Pull off necessary GT information
            # let lowercase->2D and uppercase->3D
            # [x, y, Z, W, H, L]
            gt_2d = gt_boxes3D[:, :2]
            gt_z = gt_boxes3D[:, 2]
            gt_dims = gt_boxes3D[:, 3:6]

            # this box may have been mirrored and scaled so
            # we need to recompute XYZ in 3D by backprojecting.
            gt_x3d = (
                gt_z
                * (gt_2d[:, 0] - Ks_scaled_per_box[:, 0, 2])
                / Ks_scaled_per_box[:, 0, 0]
            )
            gt_y3d = (
                gt_z
                * (gt_2d[:, 1] - Ks_scaled_per_box[:, 1, 2])
                / Ks_scaled_per_box[:, 1, 1]
            )
            gt_3d = torch.stack((gt_x3d, gt_y3d, gt_z)).T

            # put together the GT boxes
            gt_box3d = torch.cat((gt_3d, gt_dims), dim=1)

            # These are the corners which will be the target for all losses!!
            gt_corners = util.get_cuboid_verts_faces(gt_box3d, gt_poses)[0]

            # project GT corners
            gt_proj_boxes = torch.bmm(Ks_scaled_per_box, gt_corners.transpose(1, 2))
            gt_proj_boxes = gt_proj_boxes / gt_proj_boxes[:, -1, :].unsqueeze(1)

            gt_proj_x1 = gt_proj_boxes[:, 0, :].min(1)[0]
            gt_proj_y1 = gt_proj_boxes[:, 1, :].min(1)[0]
            gt_proj_x2 = gt_proj_boxes[:, 0, :].max(1)[0]
            gt_proj_y2 = gt_proj_boxes[:, 1, :].max(1)[0]

            gt_widths = gt_proj_x2 - gt_proj_x1
            gt_heights = gt_proj_y2 - gt_proj_y1
            gt_x = gt_proj_x1 + 0.5 * gt_widths
            gt_y = gt_proj_y1 + 0.5 * gt_heights

            gt_proj_boxes = torch.stack(
                (gt_proj_x1, gt_proj_y1, gt_proj_x2, gt_proj_y2), dim=1
            )

            # Compute predicted corners
            pred_cube_x3d = (
                cube_z
                * (cube_x - Ks_scaled_per_box[:, 0, 2])
                / Ks_scaled_per_box[:, 0, 0]
            )
            pred_cube_y3d = (
                cube_z
                * (cube_y - Ks_scaled_per_box[:, 1, 2])
                / Ks_scaled_per_box[:, 1, 1]
            )
            pred_cube_3D = torch.cat(
                (
                    torch.stack((pred_cube_x3d, pred_cube_y3d, cube_z)).T,
                    cube_dims,
                    cube_xy * im_ratios_per_box.unsqueeze(1),
                ),
                dim=1,
            )
            pred_verts, pred_faces = util.get_cuboid_verts_faces(
                pred_cube_3D, cube_pose
            )

            # * IMPORTANT: we need to transform results from OpenCV cs to Mitsuba CS
            cs_transform = (
                torch.tensor([[-1, 0, 0], [0, -1, 0], [0, 0, 1]]).float().to(device)
            )
            pred_verts_stacked = einops.rearrange(pred_verts, "n v d -> (n v) d")
            pred_verts_stacked = shape_utils.transform_verts(
                pred_verts_stacked, cs_transform, None
            )
            pred_verts = einops.rearrange(
                pred_verts_stacked, "(n v) d -> n v d", d=3, v=8
            ).to(device)

            pred_boxes3d = tuple([pred_verts, pred_faces])

            if self.disentangled_loss:
                """
                Disentangled loss compares each varaible group to the
                cuboid corners, which is generally more robust to hyperparams.
                """

                # compute disentangled Z corners
                cube_dis_x3d_from_z = (
                    cube_z
                    * (gt_2d[:, 0] - Ks_scaled_per_box[:, 0, 2])
                    / Ks_scaled_per_box[:, 0, 0]
                )
                cube_dis_y3d_from_z = (
                    cube_z
                    * (gt_2d[:, 1] - Ks_scaled_per_box[:, 1, 2])
                    / Ks_scaled_per_box[:, 1, 1]
                )
                cube_dis_z = torch.cat(
                    (
                        torch.stack(
                            (cube_dis_x3d_from_z, cube_dis_y3d_from_z, cube_z)
                        ).T,
                        gt_dims,
                    ),
                    dim=1,
                )
                dis_z_corners = util.get_cuboid_verts_faces(cube_dis_z, gt_poses)[0]

                # compute disentangled XY corners
                cube_dis_x3d = (
                    gt_z
                    * (cube_x - Ks_scaled_per_box[:, 0, 2])
                    / Ks_scaled_per_box[:, 0, 0]
                )
                cube_dis_y3d = (
                    gt_z
                    * (cube_y - Ks_scaled_per_box[:, 1, 2])
                    / Ks_scaled_per_box[:, 1, 1]
                )
                cube_dis_XY = torch.cat(
                    (torch.stack((cube_dis_x3d, cube_dis_y3d, gt_z)).T, gt_dims), dim=1
                )
                dis_XY_corners = util.get_cuboid_verts_faces(cube_dis_XY, gt_poses)[0]
                loss_xy = (
                    self.l1_loss(dis_XY_corners, gt_corners)
                    .contiguous()
                    .view(n, -1)
                    .mean(dim=1)
                )

                # Pose
                dis_pose_corners = util.get_cuboid_verts_faces(gt_box3d, cube_pose)[0]

                # Dims
                dis_dims_corners = util.get_cuboid_verts_faces(
                    torch.cat((gt_3d, cube_dims), dim=1), gt_poses
                )[0]

                # Loss dims
                loss_dims = (
                    self.l1_loss(dis_dims_corners, gt_corners)
                    .contiguous()
                    .view(n, -1)
                    .mean(dim=1)
                )

                # Loss z
                loss_z = (
                    self.l1_loss(dis_z_corners, gt_corners)
                    .contiguous()
                    .view(n, -1)
                    .mean(dim=1)
                )

                # Rotation uses chamfer or l1 like others
                if self.chamfer_pose:
                    loss_pose = self.chamfer_loss(dis_pose_corners, gt_corners)

                else:
                    loss_pose = (
                        self.l1_loss(dis_pose_corners, gt_corners)
                        .contiguous()
                        .view(n, -1)
                        .mean(dim=1)
                    )

            # Non-disentangled training losses
            else:
                """
                These loss functions are fairly arbitrarily designed.
                Generally, they are in some normalized space but there
                are many alternative implementations for most functions.
                """

                # XY
                gt_deltas = (
                    gt_2d.clone()
                    - torch.cat((src_ctr_x.unsqueeze(1), src_ctr_y.unsqueeze(1)), dim=1)
                ) / torch.cat(
                    (src_widths.unsqueeze(1), src_heights.unsqueeze(1)), dim=1
                )

                loss_xy = self.l1_loss(cube_2d_deltas, gt_deltas).mean(1)

                # Dims
                if self.dims_priors_enabled:
                    cube_dims_gt_normspace = torch.log(gt_dims / prior_dims)
                    loss_dims = self.l1_loss(
                        cube_dims_norm, cube_dims_gt_normspace
                    ).mean(1)

                else:
                    loss_dims = self.l1_loss(cube_dims_norm, torch.log(gt_dims)).mean(1)

                # Pose
                try:
                    if self.allocentric_pose:
                        gt_poses_allocentric = util.R_to_allocentric(
                            Ks_scaled_per_box,
                            gt_poses,
                            u=cube_x.detach(),
                            v=cube_y.detach(),
                        )
                        loss_pose = 1 - so3_relative_angle(
                            cube_pose_allocentric,
                            gt_poses_allocentric,
                            eps=0.1,
                            cos_angle=True,
                        )
                    else:
                        loss_pose = 1 - so3_relative_angle(
                            cube_pose, gt_poses, eps=0.1, cos_angle=True
                        )

                # Can fail with bad EPS values/instability
                except:
                    loss_pose = None

                if self.z_type == "direct":
                    loss_z = self.l1_loss(cube_z, gt_z)

                elif self.z_type == "sigmoid":
                    loss_z = self.l1_loss(
                        cube_z_norm, (gt_z * real_to_virtual / 100).clip(0, 1)
                    )

                elif self.z_type == "log":
                    loss_z = self.l1_loss(
                        cube_z_norm, torch.log((gt_z * real_to_virtual).clip(0.01))
                    )

                elif self.z_type == "priors":
                    loss_z = self.l1_loss(
                        cube_z_norm, (((gt_z * real_to_virtual) - z_means) / (z_stds))
                    )

            total_3D_loss_for_reporting = loss_dims * self.loss_w_dims

            if not loss_pose is None:
                total_3D_loss_for_reporting += loss_pose * self.loss_w_pose

            if not cube_2d_deltas is None:
                total_3D_loss_for_reporting += loss_xy * self.loss_w_xy

            if not loss_z is None:
                total_3D_loss_for_reporting += loss_z * self.loss_w_z

            # reporting does not need gradients
            total_3D_loss_for_reporting = total_3D_loss_for_reporting.detach()

            if self.loss_w_joint > 0:
                """
                If we are using joint [entangled] loss, then we also need to pair all
                predictions together and compute a chamfer or l1 loss vs. cube corners.
                """

                cube_dis_x3d_from_z = (
                    cube_z
                    * (cube_x - Ks_scaled_per_box[:, 0, 2])
                    / Ks_scaled_per_box[:, 0, 0]
                )
                cube_dis_y3d_from_z = (
                    cube_z
                    * (cube_y - Ks_scaled_per_box[:, 1, 2])
                    / Ks_scaled_per_box[:, 1, 1]
                )
                cube_dis_z = torch.cat(
                    (
                        torch.stack(
                            (cube_dis_x3d_from_z, cube_dis_y3d_from_z, cube_z)
                        ).T,
                        cube_dims,
                    ),
                    dim=1,
                )
                dis_z_corners_joint = util.get_cuboid_verts_faces(
                    cube_dis_z, cube_pose
                )[0]

                if self.chamfer_pose and self.disentangled_loss:
                    loss_joint = self.chamfer_loss(dis_z_corners_joint, gt_corners)

                else:
                    loss_joint = (
                        self.l1_loss(dis_z_corners_joint, gt_corners)
                        .contiguous()
                        .view(n, -1)
                        .mean(dim=1)
                    )

                valid_joint = loss_joint < np.inf
                total_3D_loss_for_reporting += (loss_joint * self.loss_w_joint).detach()

            # compute errors for tracking purposes
            z_error = (cube_z - gt_z).detach().abs()
            dims_error = (cube_dims - gt_dims).detach().abs()
            xy_error = (cube_xy - gt_2d).detach().abs()

            storage.put_scalar(
                prefix + "z_error", z_error.mean().item(), smoothing_hint=False
            )
            storage.put_scalar(
                prefix + "dims_error", dims_error.mean().item(), smoothing_hint=False
            )
            storage.put_scalar(
                prefix + "xy_error", xy_error.mean().item(), smoothing_hint=False
            )
            storage.put_scalar(
                prefix + "z_close",
                (z_error < 0.20).float().mean().item(),
                smoothing_hint=False,
            )

            storage.put_scalar(
                prefix + "total_3D_loss",
                self.loss_w_3d * self.safely_reduce_losses(total_3D_loss_for_reporting),
                smoothing_hint=False,
            )

            if self.inverse_z_weight:
                """
                Weights all losses to prioritize close up boxes.
                """

                gt_z = gt_boxes3D[:, 2]

                inverse_z_w = 1 / torch.log(gt_z.clip(E_CONSTANT))

                loss_dims *= inverse_z_w

                # scale based on log, but clip at e
                if not cube_2d_deltas is None:
                    loss_xy *= inverse_z_w

                if loss_z is not None:
                    loss_z *= inverse_z_w

                if loss_pose is not None:
                    loss_pose *= inverse_z_w

                if self.loss_w_joint > 0:
                    loss_joint *= inverse_z_w

            if self.use_confidence > 0:

                uncert_sf = SQRT_2_CONSTANT * torch.exp(-cube_uncert)

                loss_dims *= uncert_sf

                if not cube_2d_deltas is None:
                    loss_xy *= uncert_sf

                if not loss_z is None:
                    loss_z *= uncert_sf

                if loss_pose is not None:
                    loss_pose *= uncert_sf

                if self.loss_w_joint > 0:
                    loss_joint *= uncert_sf

                losses.update(
                    {
                        prefix
                        + "uncert": self.use_confidence
                        * self.safely_reduce_losses(cube_uncert.clone())
                    }
                )
                storage.put_scalar(
                    prefix + "conf",
                    torch.exp(-cube_uncert).mean().item(),
                    smoothing_hint=False,
                )

            # store per batch loss stats temporarily
            self.batch_losses = [
                batch_losses.mean().item()
                for batch_losses in total_3D_loss_for_reporting.split(
                    num_boxes_per_image
                )
            ]

            losses.update(
                {
                    prefix
                    + "loss_dims": self.safely_reduce_losses(loss_dims)
                    * self.loss_w_dims
                    * self.loss_w_3d,
                }
            )

            if not cube_2d_deltas is None:
                losses.update(
                    {
                        prefix
                        + "loss_xy": self.safely_reduce_losses(loss_xy)
                        * self.loss_w_xy
                        * self.loss_w_3d,
                    }
                )

            if not loss_z is None:
                losses.update(
                    {
                        prefix
                        + "loss_z": self.safely_reduce_losses(loss_z)
                        * self.loss_w_z
                        * self.loss_w_3d,
                    }
                )

            if loss_pose is not None:

                losses.update(
                    {
                        prefix
                        + "loss_pose": self.safely_reduce_losses(loss_pose)
                        * self.loss_w_pose
                        * self.loss_w_3d,
                    }
                )

            if self.loss_w_joint > 0:
                if valid_joint.any():
                    losses.update(
                        {
                            prefix
                            + "loss_joint": self.safely_reduce_losses(
                                loss_joint[valid_joint]
                            )
                            * self.loss_w_joint
                            * self.loss_w_3d
                        }
                    )

            return losses, pred_boxes3d

        else:
            """
            Inference
            """
            if len(cube_z.shape) == 0:
                cube_z = cube_z.unsqueeze(0)

            # inference
            cube_x3d = (
                cube_z
                * (cube_x - Ks_scaled_per_box[:, 0, 2])
                / Ks_scaled_per_box[:, 0, 0]
            )
            cube_y3d = (
                cube_z
                * (cube_y - Ks_scaled_per_box[:, 1, 2])
                / Ks_scaled_per_box[:, 1, 1]
            )
            cube_3D = torch.cat(
                (
                    torch.stack((cube_x3d, cube_y3d, cube_z)).T,
                    cube_dims,
                    cube_xy * im_ratios_per_box.unsqueeze(1),
                ),
                dim=1,
            )

            if self.use_confidence:
                cube_conf = torch.exp(-cube_uncert)
                cube_3D = torch.cat((cube_3D, cube_conf.unsqueeze(1)), dim=1)

            # convert the predictions to intances per image
            cube_3D = cube_3D.split(num_boxes_per_image)
            cube_pose = cube_pose.split(num_boxes_per_image)

            for cube_3D_i, cube_pose_i, instances_i, K, im_dim, im_scale_ratio in zip(
                cube_3D, cube_pose, instances, Ks, im_current_dims, im_scales_ratio
            ):

                instances_i.scores = (instances_i.scores * cube_3D_i[:, -1]) ** (1 / 2)
                verts_i, faces_i = util.get_cuboid_verts_faces(
                    cube_3D_i[:, :6], cube_pose_i
                )

                # * IMPORTANT: we need to transform results from OpenCV cs to Mitsuba CS
                cs_transform = (
                    torch.tensor([[-1, 0, 0], [0, -1, 0], [0, 0, 1]]).float().to(device)
                )
                verts_i_stacked = einops.rearrange(verts_i, "n v d -> (n v) d")
                verts_i_stacked = shape_utils.transform_verts(
                    verts_i_stacked, cs_transform, None
                )
                verts_i = einops.rearrange(
                    verts_i_stacked, "(n v) d -> n v d", d=3, v=8
                ).to(device)

                instances_i.pred_bbox3D = verts_i
                instances_i.pred_bbox3D_faces = faces_i
                instances_i.pred_center_cam = cube_3D_i[:, :3]
                instances_i.pred_center_2D = cube_3D_i[:, 6:8]
                instances_i.pred_dimensions = cube_3D_i[:, 3:6]
                instances_i.pred_pose = cube_pose_i

            return instances

    def _sample_proposals(
        self,
        matched_idxs: torch.Tensor,
        matched_labels: torch.Tensor,
        gt_classes: torch.Tensor,
        matched_ious=None,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Based on the matching between N proposals and M groundtruth,
        sample the proposals and set their classification labels.
        Args:
            matched_idxs (Tensor): a vector of length N, each is the best-matched
                gt index in [0, M) for each proposal.
            matched_labels (Tensor): a vector of length N, the matcher's label
                (one of cfg.MODEL.ROI_HEADS.IOU_LABELS) for each proposal.
            gt_classes (Tensor): a vector of length M.
        Returns:
            Tensor: a vector of indices of sampled proposals. Each is in [0, N).
            Tensor: a vector of the same length, the classification label for
                each sampled proposal. Each sample is labeled as either a category in
                [0, num_classes) or the background (num_classes).
        """
        has_gt = gt_classes.numel() > 0
        # Get the corresponding GT for each proposal
        if has_gt:
            gt_classes = gt_classes[matched_idxs]
            # Label unmatched proposals (0 label from matcher) as background (label=num_classes)
            gt_classes[matched_labels == 0] = self.num_classes
            # Label ignore proposals (-1 label)
            gt_classes[matched_labels == -1] = -1
        else:
            gt_classes = torch.zeros_like(matched_idxs) + self.num_classes

        sampled_fg_idxs, sampled_bg_idxs = subsample_labels(
            gt_classes,
            self.batch_size_per_image,
            self.positive_fraction,
            self.num_classes,
            matched_ious=matched_ious,
        )

        sampled_idxs = torch.cat([sampled_fg_idxs, sampled_bg_idxs], dim=0)
        return sampled_idxs, gt_classes[sampled_idxs]

    @torch.no_grad()
    def _label_and_sample_proposals(
        self, proposals: List[Instances], targets: List[Instances]
    ) -> List[Instances]:
        # ! we overwrite this since we only have valid annos! The default function does the job in this case

        # separate valid and ignore gts
        targets_ign = [target[target.gt_classes < 0] for target in targets]
        targets = [target[target.gt_classes >= 0] for target in targets]

        if self.proposal_append_gt:
            proposals = add_ground_truth_to_proposals(targets, proposals)

        proposals_with_gt = []

        num_fg_samples = []
        num_bg_samples = []

        for proposals_per_image, targets_per_image, targets_ign_per_image in zip(
            proposals, targets, targets_ign
        ):

            has_gt = len(targets_per_image) > 0

            match_quality_matrix = pairwise_iou(
                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes
            )
            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)

            try:
                if len(targets_ign_per_image) > 0:

                    # compute the quality matrix, only on subset of background
                    background_inds = (matched_labels == 0).nonzero().squeeze()

                    # determine the boxes inside ignore regions with sufficient threshold
                    if background_inds.numel() > 1:
                        match_quality_matrix_ign = pairwise_ioa(
                            targets_ign_per_image.gt_boxes,
                            proposals_per_image.proposal_boxes[background_inds],
                        )
                        matched_labels[
                            background_inds[
                                match_quality_matrix_ign.max(0)[0] >= self.ignore_thresh
                            ]
                        ] = -1

                        del match_quality_matrix_ign
            except:
                pass

            gt_arange = torch.arange(match_quality_matrix.shape[1]).to(
                matched_idxs.device
            )
            matched_ious = match_quality_matrix[matched_idxs, gt_arange]
            sampled_idxs, gt_classes = self._sample_proposals(
                matched_idxs,
                matched_labels,
                targets_per_image.gt_classes,
                matched_ious=matched_ious,
            )

            # Set target attributes of the sampled proposals:
            proposals_per_image = proposals_per_image[sampled_idxs]
            proposals_per_image.gt_classes = gt_classes

            if has_gt:
                sampled_targets = matched_idxs[sampled_idxs]
                # We index all the attributes of targets that start with "gt_"
                # and have not been added to proposals yet (="gt_classes").
                # NOTE: here the indexing waste some compute, because heads
                # like masks, keypoints, etc, will filter the proposals again,
                # (by foreground/background, or number of keypoints in the image, etc)
                # so we essentially index the data twice.
                for (trg_name, trg_value) in targets_per_image.get_fields().items():
                    if trg_name.startswith("gt_") and not proposals_per_image.has(
                        trg_name
                    ):
                        proposals_per_image.set(trg_name, trg_value[sampled_targets])

            num_bg_samples.append((gt_classes == self.num_classes).sum().item())
            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])
            proposals_with_gt.append(proposals_per_image)

        # Log the number of fg/bg samples that are selected for training ROI heads
        storage = get_event_storage()
        storage.put_scalar("roi_head/num_fg_samples", np.mean(num_fg_samples))
        storage.put_scalar("roi_head/num_bg_samples", np.mean(num_bg_samples))

        return proposals_with_gt

    def safely_reduce_losses(self, loss):

        valid = (~(loss.isinf())) & (~(loss.isnan()))

        if valid.any():
            return loss[valid].mean()
        else:
            # no valid losses, simply zero out
            return loss.mean() * 0.0
